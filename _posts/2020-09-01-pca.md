---
layout: post
title: pca
date: 2020-09-01 23:49 -0700
---

The definition of PCA is the orthogonal projection of the data onto a lower dimensional linear space, such that the variance of the projected data is maximized.

Assuming we have $m$ data points in our data set, and each data point is a $n$-dim vector. The mean vector for $n$ features are defined as following:

$$
    \mathbf{\bar{x}} = \frac{1}{m}\Sigma_{i}^{m}\mathbf{x}_i
$$

Let us define $\mu_1$ as the direction of projection, which is a unit vector, so that $\mu_1^\top\mu_1=1$. The variance of data ($\mathbf{x}_i-\mathbf{\bar{x}}$) after zero-centered and projected on $\mu_1$ is $(\mathbf{x}_i-\mathbf{\bar{x}})^\top\mu_1$, the total variance of data is given by the following:

$$
   J = \frac{1}{m}\cdot\Sigma_{i}^{m}((\mathbf{x}_i-\mathbf{\bar{x}})^\top\mu_1)^2 \\
   J = \frac{1}{m}\cdot(\mathbf{x^\prime}^\top\mu_1)^2 \\
   J = \frac{1}{m}\cdot(\mathbf{x^\prime}^\top\mu_1)^T(\mathbf{x^\prime}^\top\mu_1) \\
   J = \frac{1}{m}\cdot\mu_1^\top\mathbf{x^\prime}\mathbf{x^\prime}^\top\mu_1
$$

where $\mathbf{x^\prime}$ is the zero-centered data. Let $\mathbf{S}=\frac{1}{m}\cdot\mathbf{x^\prime}\mathbf{x^\prime}^\top$, the Eq(2) is converted to:

$$
    J = \mu_1^\top\mathbf{S}\mu_1
$$

Since the constraint: $\mu_1^\top\mu_1=1$, after introducing a Lagrange multiplier, and then make an unconstrained maximization of:

$$
    \tilde{J} = \mu_1^\top\mathbf{S}\mu_1 + \lambda_1(1-\mu_1^\top\mu_1) \\
    \frac{\partial\tilde{J}}{\partial\mu_1} = (\mathbf{S} + \mathbf{S}^\top)\mu_1-2\lambda_1\mu_1 = 2\mathbf{S}\mu_1 - 2\lambda_1\mu_1 = 0
$$

which says that $\mu_1$ must be an eigenvector of $\mathbf{S}$, and we left-multiply by $\mu_1^\top$ and make use of $\mu_1^\top\mu_1=1$, we see that the variance is given by:

$$
    \mu_1^\top\mathbf{S}\mu_1 = \lambda_1
$$

So the variance will be a maximum when we set $\mu_1$ equal to the eigenvector having the largest eigenvalue $\lambda_1$

Let's dive into some python snippets to show PCA implementations.

```python
import numpy as np

# construct data points
x1 = [2, 4, 1, 5]
x2 = [3, 2, 7, 5]
x3 = [9, 3, 8, 2]
_X = np.asarray([x1,x2,x3])

# zero-meaned data
X = _X - np.mean(_X, axis=0)
```

Since $X$ is zero-meaned, we can directly use $\frac{1}{m-1}X^\top X$ to compute covariance. $m$ is the number of data points. Use `np.cov`, we do not need zero-mean first. Actually, whether zero-meaned or not, it won't affect the covariance.  

```python
cov_mat0 = np.matmul(X.T, X) / (X.shape[0] - 1)
cov_mat1 = np.cov(_X, rowvar=False)
```
`cov_mat0` should be equal to `cov_mat1`.

##### 1) Use `svd` on covariance matrix
```python
U, S, V = np.linalg.svd(cov_mat0)
U, S
# output
(array([[-0.6838271 , -0.54800188,  0.23460994,  0.42075244],
        [ 0.0893389 , -0.37316815, -0.90845985,  0.16572505],
        [-0.66134448,  0.66745155, -0.34192161, -0.01488372],
        [ 0.29499584,  0.33903967,  0.05242605,  0.89178532]]),
 array([2.70350985e+01, 5.63156816e+00, 1.10027031e-15, 2.98724660e-16]))
```
`U` is the eigenvector of covariance matrix. `S` is the corresponding eigenvalues.

##### 2) Use `eig` on covariance matrix
```python
eigVal, eigVec = np.linalg.eig(cov_mat0)
eigVec, eigVal
# output
(array([[-0.6838271 , -0.54800188,  0.35663984, -0.1118191 ],
        [ 0.0893389 , -0.37316815, -0.80803817, -0.78118693],
        [-0.66134448,  0.66745155, -0.32878603, -0.24175913],
        [ 0.29499584,  0.33903967,  0.33433828, -0.56462022]]),
 array([ 2.70350985e+01,  5.63156816e+00,  7.39044900e-16, -2.65008465e-16]))
```
`eigenVec` is the eigenvector of covariance, and `eigenVal` is the corresponding eigenvalues.  

As we can see, the result of **SVD** and **Eigen decomposition** is exactly the same. The reason is that covariance is symmetric positive semi-definite matrix

##### 3) Use `svd` directly on `X`
```python
U1, S1, V1 = np.linalg.svd(X)
np.square(S1) / (X.shape[0] - 1), V1.T
# output
(array([2.70350985e+01, 5.63156816e+00, 1.51981102e-31]),
 array([[-0.6838271 , -0.54800188,  0.44383838,  0.18730171],
        [ 0.0893389 , -0.37316815, -0.6141324 ,  0.68964157],
        [-0.66134448,  0.66745155, -0.27868386,  0.19866357],
        [ 0.29499584,  0.33903967,  0.59007134,  0.67070513]]))
```
The result shows that if we directly apply `svd` on `X`, the eigenvalue will be the square of covariance's eigenvalues divided by $m-1$, $m$ is the number of data points. And the transpose of V is equal to the corresponding eigenvectors of the covariance.

The result is quite easy to proof. Assuming we have:
$$
  X = U\Sigma V^\top
$$
And we left-multiply $X^\top$ to both side of equation.

$$
  X^\top X = X^\top U\Sigma V^\top \\
  X^\top X = V\Sigma^\top U^\top U\Sigma V^\top \\
  X^\top X = V\Sigma^2V^\top
$$

And $X^\top X$ is the covariance of data matrix (Here we leave out $(m-1)$ for simplicity).


#### Some tricks:

1. According to the definiton of $\mathbf{S}$, it is the covariance matrix of data set. So we could apply either svd or eigendecomposition algorithms to solve it. (Difference between svd and eigendecomposition if the matrix is not symmetrical.)

2. In practice, we prefer to use svd instead of eigendecomposition to implement PCA for two reasons:
    1. We don't need compute $X^\top X$ this step, it will lost some small values during the computation, like Lauchli matrix.
    2. SVD is faster than eigendecomposition in many implementations.

3. As we can see, zero-centered is not part of pre-processing of data. On the contrary, it is part of PCA algorithm.






### PCA Summary:
1. Evaluating the mean $\bar{x}$ and the covariance matrix $\mathbf{S}$ of the data set.
2. Finding the $M$ eigenvectors of $\mathbf{S}$ corresponding to the $M$ largest eigenvalues.

### Reference
[Pattern Recognition and Machine Learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)

[Derivatives of matrix and vector](https://daiwk.github.io/assets/matrix+vector+derivatives+for+machine+learning.pdf)

[降维方法PCA与SVD的联系与区别](https://www.cnblogs.com/bjwu/p/9280492.html)

[从PCA和SVD的关系拾遗](https://blog.csdn.net/Dark_Scope/article/details/53150883)
